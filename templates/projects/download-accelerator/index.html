{% extends "projects/base/index_base.html" %}

{% from "macros/links.html" import wikilink, githublink %}
{% from "macros/math.html" import include_mathjax %}

{% block title %}Download Accelerator{% endblock %}
{% block head %}

    {{ super() }}

    {{ include_mathjax() }}

{% endblock %}
{% block projectSummary %}

    <p>
        {{ wikilink('Rust', 'rust (programming language)') }} is an up-and-coming
        systems programming language developed by {{ wikilink('Mozilla') }}, the
        organization behind the {{ wikilink('Firefox') }} web browser. I first heard
        of Rust around the onset of 2016, and was mildly curious. See, at the time
        the extent of my experience with systems programming was
        limited to just
        {{ wikilink('C', 'c (programming language)') }} / {{ wikilink('C++') }},
        and I was glad to see that there is some effort underway to rectify these
        their weak-points through a brand new language. However, it is only now
        (May of 2017) that I have finally decided to buckle down and start investing
        some time in learning it.
    </p>
    <p>
        After giving the
        <a href="https://rust-lang.github.io/book/second-edition/index.html"
           target="_blank">
            Rust Book (2nd edition)
        </a>
        a not so brief read, I set out to test my understanding of Rust
        by producing a small-ish sized project application. Combining that
        with my unrelated desire to refresh my dwindling memory of what I know
        about computer networking, I decided to kill two birds with one stone by
        implementing a bare-bones downloader of web content. To up the difficulty
        just a notch, I also wished it to fetch and process multiple parts
        of the target content in parallel, thereby cutting down overall download
        time to a fraction of its original value.
    </p>
    <p>
        I gave the application the
        code name 'Squid Downloader', and drew this handsome logo to accompany it:
    </p>
    <div class="figure">
<pre>
.. . :. ..:. :...:
: o.  .. :..:. :.
:. .:: ---- .:. :.
. : . / O O\ .: o
:. o .\    / : . .
 : ..:///\\\  o .
: . : \\\/// o .:.
. :.. ///\\\ ..: :
: . o \\\///. :. o
.: : o /.\\. :. ..
: .:. : : / :. :.:
</pre>
    </div>

{% endblock %}
{% block tableOfContents %}

    <h2>Table of contents</h2>

    <ul>
        <li><a href="#section-approach">Approach</a></li>
        <li><a href="#section-source-code">Source code</a></li>
        <li><a href="#section-see-also">See also</a></li>
    </ul>

{% endblock %}
{% block content %}

    {{ super() }}

    <a id="section-approach"></a>
    <h2>Approach</h2>

    <p>
        Simplified, a 'download' is the request for and then transfer of data
        from a <b>server</b> to a <b>client</b>. The data being transferred does
        not traverse the medium between the server and client as one homogeneous
        unit, but rather as a <b>stream</b> of little pieces (more commonly known
        as {{ wikilink('packets', 'network packet') }}) arriving one after
        the other.
    </p>
    <p>
        The most basic download strategy consists of (not surprisingly)
        a single such stream, that over time transfers the entirety of
        the target data <b>content</b> to the client. However, if we introduce
        the possibility of concurrent operations to the equation, then a more
        appealing strategy becomes available.
    <p>
        First, regard the target content as a sequence of elementary units of
        information (when the content is digital it is often convenient to use
        {{ wikilink('bytes', 'byte') }}, and so we shall).
        Now, given that the target content \(T\) is made up of \(n\) bytes in sequence
        \((x_1, x_2, \ldots x_n)\), divide it into a set \(C\) of \(m\) chunks
        taking the form:
    </p>
    <div class="math">
        $$ R(i, j) = (x_i, x_{i+1}, \ldots x_{j-1}) \text{ with } i < j $$
        $$ C = \{R(1, a_1), R(a_1, a_2), \ldots R(a_{m-1}, n+1) \} $$
    </div>
    <p>
        Fetching data in a single stream would then be equivalent to "dividing"
        \(T\) into a single chunk \(R(1, n+1)\) and requesting that from the server.
        But with concurrency available to us, we can divide \(T\) into several chunks
        \(c_1, c_2, \ldots c_m\) and retrieve their contents through several streams
        simultaneously.
        Once the data has arrived at the client, all that is left to do is to glue these
        chunks back together in the correct order to reproduce the original
        sequence of bytes of the target content.
    </p>
    <p>
        {{ wikilink('HTTP') }} does allow for clients to request ranges
        of the target content by including the
        <a href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.16"
           target="_blank">
            relevant header
        </a> in the request, so why is it not possible for a client to have \(m\)
        be arbitrarily large (up to \(m = n\))
        to request many bytes in parallel? Well, technically it is possible, but
        in practice pursuing such greed is futile for many reasons,
        with these here being the principal ones:
    </p>
    <ol>
        <li>
            The network's data throughput is insufficient.
        </li>
        <li>
            The client/server machine's processing power is insufficient.
        </li>
        <li>
            The server forbids clients from making too many requests in a short
            interval of time (to prevent abuse or because such a service is
            reserved for specific clients).
        </li>
        <li>
            The server refuses to open too many streams concurrently to the same
            client (again, to prevent abuse or because such a service is reserved
            for specific clients).
        </li>
    </ol>
    <p>
        Therefore, in practice the number of open concurrent streams at a time
        is rather modest, but the benefit of partial content fetching is still
        sometimes noticeable. For example: while testing
        the application during development, in one instance I measured a 50%
        reduction in download time. Of course, the degree of improvement varies on
        a case-by-case basis,
        depending on the client and server machines, the network infrastructure
        connecting them, and the content being requested.
    </p>

    <a id="section-source-code"></a>
    <h2>Source code</h2>

    <p>
        You can view the source code on
        {{ githublink('GitHub', 'rharel', 'rust-squid-downloader') }}.
        Documentation is hosted
        <a href="https://rharel.github.io/rust-squid-downloader/squid/index.html"
           target="_blank">
            over here</a>.
    </p>

    <a id="section-see-also"></a>
    <h2>See also</h2>

    <ul>
        <li>
            The
            <a href="https://rust-lang.github.io/book/second-edition/index.html"
               target="_blank">
                Rust Book (2nd edition)
            </a>
            if you wish to learn more about Rust.
        </li>
        <li>
            The
            <a href="https://tools.ietf.org/html/rfc7233#section-3.1"
               target="_blank">

                official specification
            </a>
            of the HTTP Range request header.
        </li>
    </ul>

{% endblock %}
{% block lastUpdateStamp %} May, 2017 {% endblock %}
